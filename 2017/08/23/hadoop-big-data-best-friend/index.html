<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Hadoop - Big Data's Best Friend · Brainstain</title><meta name="description" content="Hadoop - Big Data's Best Friend - Luyao &quot;Lulu&quot; Wang &amp; John L. Bernstein IV"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://luyao-wang.github.io/atom.xml" title="Brainstain"><script src="https://use.fontawesome.com/0a81c537eb.js">   </script><script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/tags/" target="_self" class="nav-list-link">TAG</a></li><li class="nav-list-item"><a href="/cs-3240/" target="_self" class="nav-list-link">CS_3240</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/contact/" target="_self" class="nav-list-link">CONTACT</a></li><li class="nav-list-item"><a href="https://github.com/luyao-wang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/search" target="_self" class="nav-list-link search"><i class="fa fa-search" aria-hidden="true"></i></a></li></ul></header><main class="container"></main><div class="post"><article class="post-block"><h1 class="post-title">Hadoop - Big Data's Best Friend</h1><div class="post-info">Aug 23, 2017<span class="categories"><i class="fa fa-bookmark" aria-hidden="true"></i></span><a href="/categories/程序媛💁‍技术贴/" class="post-category">#程序媛💁‍技术贴</a><a href="/categories/程序媛💁‍技术贴/en-🇺🇸/" class="post-category">#en 🇺🇸</a><a href="/categories/程序媛💁‍技术贴/en-🇺🇸/school🎓/" class="post-category">#school🎓</a><a href="/categories/程序媛💁‍技术贴/en-🇺🇸/school🎓/theory/" class="post-category">#theory</a></div><div class="post-content"><p><img src="https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAcYAAAAJDgwYjA0ZmViLTJiNzgtNGJmMS1iNjE0LWQ3MzhiZmNjNzNhMg.png" alt=""></p>
<h3 id="What-is-Hadoop"><a href="#What-is-Hadoop" class="headerlink" title="What is Hadoop?"></a>What is Hadoop?</h3><p>Hadoop is a framework of MapReduce utilized for handling “Big Data” operations and storage.</p>
<ul>
<li>MapReduce is a programming model utilized for handling database clusters (separate computers that work in unison with one another for a central task)</li>
</ul>
<p>Hadoop is broken up into four main components:</p>
<ul>
<li><p>Hadoop Common (standard library for Hadoop)</p>
</li>
<li><p>HDFS (Hadoop Distributed File System)</p>
</li>
<li><p>MapReduce Engine (JobTracker &amp; TaskTracker)</p>
</li>
<li><p>YARN (Yet Another Resource Negotiator)</p>
</li>
</ul>
<h3 id="Hadoop-Common"><a href="#Hadoop-Common" class="headerlink" title="Hadoop Common"></a>Hadoop Common</h3><p>Hadoop Common is the standard library for Hadoop.</p>
<p>Previously titled “Hadoop Core”, the name was changed to “Hadoop Common” in July 2009.</p>
<ul>
<li>The name was changed because it’s the list of common libraries and utilities used by Hadoop. Not the most exciting reasons for the name change.</li>
</ul>
<p>Hadoop Common also includes the necessary Java Archive (.jar) files that Hadoop needs to work with the Java Virtual Machine.</p>
<p>Hadoop Common also includes several codecs utilized for compression (e.g. bzip2).</p>
<p>For a full overview of Hadoop Common, see <a href="https://github.com/apache/hadoop-common" target="_blank" rel="external">this mirror</a> from Github: <a href="https://github.com/apache/hadoop-common" target="_blank" rel="external">https://github.com/apache/hadoop-common</a></p>
<h3 id="Hadoop’s-Distributed-File-System-HDFS"><a href="#Hadoop’s-Distributed-File-System-HDFS" class="headerlink" title="Hadoop’s Distributed File System (HDFS)"></a>Hadoop’s Distributed File System (HDFS)</h3><p><img src="https://punekaramit.files.wordpress.com/2014/06/hadoop-overview.png" alt=""></p>
<p>NameNode is a special server for storing metadata on files.</p>
<p>Files themselves stored in DataNode.</p>
<p>NameNode also contains maps to direct metadata from each file to an appropriate data block in the DataNode servers.</p>
<p>Each cluster in Hadoop has its own NameNode with the current build of Hadoop.</p>
<p>Secondary NameNode acts as a helper to create checkpoints (keeps track of what data has been read, modified, etc) for NameNode. It is NOT a backup.</p>
<p>However Secondary NameNode is used for protection from corruption in the event that [first] NameNode is ever dropped. Actual backups must be handled by the database administrator(s) themselves instead of being built into Hadoop.</p>
<h3 id="MapReduce-Engine-JobTracker-amp-TaskTracker"><a href="#MapReduce-Engine-JobTracker-amp-TaskTracker" class="headerlink" title="MapReduce Engine - JobTracker &amp; TaskTracker"></a>MapReduce Engine - JobTracker &amp; TaskTracker</h3><p><strong>JobTracker (JT)</strong></p>
<p>JT takes in tasks from client, then delegates information out to a corresponding TaskTracker service workers.</p>
<p>JT also monitors status of jobs from corresponding TT service workers, then transmits status to the client.</p>
<p>If the JT of a NameNode goes down, HDFS will still be able to work but MapReduce executions will be stopped and halted. No further MapReduce executions will be able to be sent to JobTracker to delegate to TaskTracker service workers.</p>
<p><strong>TaskTracker (TT)</strong></p>
<p>Built to handle Mapper and Reducer jobs delegated by the JobTracker that summoned it.</p>
<ul>
<li><p>Maps are just key-value pairs of transformed data. E.g. [1,2,3,4] mapped with * 2 = [2,4,6,8];</p>
</li>
<li><p>Reducers are transformations of records with similar keys down into a smaller set.</p>
</li>
</ul>
<p>TaskTrackers are slaves to their summoning JT. In the event a TT goes down, JT will simply reassign the task to a new TaskTracker.</p>
<h3 id="TaskTracker-Overview-of-MapReduce-call-chain"><a href="#TaskTracker-Overview-of-MapReduce-call-chain" class="headerlink" title="TaskTracker Overview of MapReduce call chain"></a>TaskTracker Overview of MapReduce call chain</h3><p><img src="https://examples.javacodegeeks.com/wp-content/uploads/2015/11/MapReduce.jpg" alt=""></p>
<p>The example above is a diagram of how a word count program in Hadoop which map words in a file together (finding all instances of the word) and then reduces the amount of found words into a single value (the actual number of times the word appears in the file).</p>
<p>Found at: <a href="https://examples.javacodegeeks.com/enterprise-java/apache-hadoop/hadoop-hello-world-example/" target="_blank" rel="external">https://examples.javacodegeeks.com/enterprise-java/apache-hadoop/hadoop-hello-world-example/</a></p>
<h3 id="YARN-Yet-Another-Resource-Negotiator"><a href="#YARN-Yet-Another-Resource-Negotiator" class="headerlink" title="YARN (Yet Another Resource Negotiator)"></a>YARN (Yet Another Resource Negotiator)</h3><p>Used to decouple resource management from actual MapReduce jobs.</p>
<p>YARN works by using a central Resource Manager that contacts individual “Node Managers” that exist on each separate machine in the cluster (the “nodes” of the cluster).</p>
<ul>
<li><p>Node Managers manages containers on each node.</p>
</li>
<li><p>Application Master (App Mstr) is used to communicate information about resources between the Resource Manager and individual Node Managers.</p>
</li>
</ul>
<p>The Resource Manager also utilizes a Scheduler to schedule requests with individual Node Managers through the App Master.</p>
<ul>
<li><p>Note that the Scheduler does not deal with fault tolerance. It merely keeps track of resources. Fault tolerance is instead handled via the individual Node Manager of each machine.</p>
</li>
<li><p>Fault tolerance is also handled by particular Hadoop frameworks (e.g. HortonWorks).</p>
</li>
<li><p>Data Replication also used for fault tolerance.</p>
</li>
</ul>
<p><img src="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif" alt=""></p>
<h3 id="Fault-Tolerance-Hadoop-HA"><a href="#Fault-Tolerance-Hadoop-HA" class="headerlink" title="Fault Tolerance / Hadoop HA"></a>Fault Tolerance / Hadoop HA</h3><p>NameNode serves as single point of failure for Hadoop. If NameNode drops, cluster access becomes unavailable.</p>
<p>Hadoop High Availability aims to relieve NameNode as SPOF via Quorum Journal Manager (QJM).</p>
<p>Second NameNode used as slave to first NameNode in a “standby mode” via “JournalNodes” (JNs); JNs are used to correspond client operations.</p>
<p>To avoid possible data corruption, HA cluster enforces only allow a single active NameNode to be the “writer” to JournalNodes. Data corruption can arise from “split-brain scenario” (nodes in cluster believe they are the only active node).</p>
<p><img src="https://knoldus.files.wordpress.com/2017/06/namenode.png?w=660" alt=""></p>
<h3 id="Hadoop-Streaming-–-Performance-amp-Overhead"><a href="#Hadoop-Streaming-–-Performance-amp-Overhead" class="headerlink" title="Hadoop Streaming – Performance &amp; Overhead"></a>Hadoop Streaming – Performance &amp; Overhead</h3><p>Hadoop natively works utilizing Java but offers support to work with other languages (e.g. Python, Ruby, etc) via Hadoop Streaming.</p>
<p>Hadoop Streaming provides more abstract implementation of Hadoop for generating jobs for Hadoop, there is a performance overhead that makes Hadoop Streaming a more costly implementation over using Java.</p>
<ul>
<li><p>Overhead bottleneck comes from implementation of Linux pipe feature.</p>
</li>
<li><p>Linux pipe() system call is half-duplex, so two pipes must be opened to read and write data between Hadoop filesystem and external executable file.</p>
</li>
</ul>
<p>Java I/O operations rewrite map pairs to external executables which in turn leads to a performance overhead. (Zheng et. al).</p>
<p><strong><code>Breakdown of Hadoop Streaming Overhead</code></strong>
<img src="https://www.researchgate.net/profile/Song_Guo6/publication/239761247/figure/fig3/AS:393244333625357@1470768163687/Figure-3-The-copying-path-of-Key-Value-pairs-in-Hadoop-Streaming-Each-Key-Value.png" alt=""></p>
<h3 id="Security-HDFS’s-Vulnerabilities"><a href="#Security-HDFS’s-Vulnerabilities" class="headerlink" title="Security: HDFS’s Vulnerabilities"></a>Security: HDFS’s Vulnerabilities</h3><p>HDFS supports no authentication model in its raw form. Instead relies upon Unix-level authentication &amp; authorization.</p>
<ul>
<li>As such, HDFS is open to several types of attacks, ranging from impersonation, cluster head bypass attacks, and eavesdropping attacks.</li>
</ul>
<p>No encryption of sensitive data in HDFS’s vanilla state.</p>
<ul>
<li>“Heavy weight cryptographic operations” are left out due to high overhead on performance.</li>
</ul>
<p>Security measures are dependent upon company policy and use of security systems such as Kerberos.</p>
<h3 id="Sources-–-Online-Information"><a href="#Sources-–-Online-Information" class="headerlink" title="Sources – Online Information"></a>Sources – Online Information</h3><ul>
<li><p>What is Hadoop? - <a href="https://en.wikipedia.org/wiki/Apache_Hadoop" target="_blank" rel="external">https://en.wikipedia.org/wiki/Apache_Hadoop</a></p>
</li>
<li><p>HDFS - <a href="http://www.aosabook.org/en/hdfs.html" target="_blank" rel="external">http://www.aosabook.org/en/hdfs.html</a></p>
</li>
<li><p>HDFS Checkpoints - <a href="https://blog.cloudera.com/blog/2014/03/a-guide-to-checkpointing-in-hadoop/" target="_blank" rel="external">https://blog.cloudera.com/blog/2014/03/a-guide-to-checkpointing-in-hadoop/</a></p>
</li>
<li><p>Secondary NameNode - <a href="http://hadooptutorial.info/secondary-namenode-in-hadoop/" target="_blank" rel="external">http://hadooptutorial.info/secondary-namenode-in-hadoop/</a></p>
</li>
<li><p>Hadoop Common - <a href="https://github.com/apache/hadoop-common" target="_blank" rel="external">https://github.com/apache/hadoop-common</a></p>
</li>
<li><p>MapReduce Engine’s Processes - <a href="http://hadoopinrealworld.com/jobtracker-and-tasktracker/" target="_blank" rel="external">http://hadoopinrealworld.com/jobtracker-and-tasktracker/</a></p>
</li>
<li><p>YARN - <a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="external">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html</a></p>
</li>
<li><p>Hadoop HA / QJM - <a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html</a></p>
</li>
<li><p>Security -<a href="https://steveloughran.gitbooks.io/kerberos_and_hadoop/content/sections/the_limits_of_hadoop_security.html" target="_blank" rel="external">https://steveloughran.gitbooks.io/kerberos_and_hadoop/content/sections/the_limits_of_hadoop_security.html</a></p>
</li>
</ul>
<h3 id="Sources-–-Papers"><a href="#Sources-–-Papers" class="headerlink" title="Sources – Papers"></a>Sources – Papers</h3><p>[1] Fault Tolerance / Hadoop HA - Wang, F., Qiu, J., Yang, J., Dong, B., Li, X., &amp; Li, Y. (2009). Hadoop high availability through metadata replication. Proceeding of the first international workshop on Cloud data management - CloudDB 09. doi:10.1145/1651263.1651271</p>
<p>[2] Hadoop Security &amp; Vulnerabilities - Sarvabhatla, M., Reddy, M. C., &amp; Vorugunti, C. S. (2015). A Robust and Light Weight Authentication Framework for Hadoop File System in Cloud Computing Environment. Proceedings of the Third International Symposium on Women in Computing and Informatics - WCI 15, 463-468. doi:10.1145/2791405.2791410</p>
<p>[3] Hadoop Streaming - Ding, M., Zheng, L., Lu, Y., Li, L., Guo, S., &amp; Guo, M. (2011). More Convenient More Overhead: The Performance Evaluation of Hadoop Streaming. Proceedings of the 2011 ACM Symposium on Research in Applied Computation - RACS 11, 307-313. doi:10.1145/2103380.2103444</p>
<h3 id="Sources-Images"><a href="#Sources-Images" class="headerlink" title="Sources - Images"></a>Sources - Images</h3><ul>
<li><p>Hadoop Logo - <a href="https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAcYAAAAJDgwYjA0ZmViLTJiNzgtNGJmMS1iNjE0LWQ3MzhiZmNjNzNhMg.png" target="_blank" rel="external">https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAcYAAAAJDgwYjA0ZmViLTJiNzgtNGJmMS1iNjE0LWQ3MzhiZmNjNzNhMg.png</a></p>
</li>
<li><p>HDFS Overview - <a href="https://punekaramit.files.wordpress.com/2014/06/hadoop-overview.png" target="_blank" rel="external">https://punekaramit.files.wordpress.com/2014/06/hadoop-overview.png</a></p>
</li>
<li><p>TaskTracker Overview - <a href="https://examples.javacodegeeks.com/wp-content/uploads/2015/11/MapReduce.jpg" target="_blank" rel="external">https://examples.javacodegeeks.com/wp-content/uploads/2015/11/MapReduce.jpg</a></p>
</li>
<li><p>YARN Architecture - <a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif" target="_blank" rel="external">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif</a></p>
</li>
<li><p>QJM Overview - <a href="https://knoldus.files.wordpress.com/2017/06/namenode.png?w=660" target="_blank" rel="external">https://knoldus.files.wordpress.com/2017/06/namenode.png?w=660</a></p>
</li>
</ul>
</div><p class="post-tags"><i class="fa fa-tags" aria-hidden="true"></i><a href="/tags/hadoop/">#hadoop</a><a href="/tags/big-data/">#big data</a><a href="/tags/HDFS/">#HDFS</a><a href="/tags/MapReduce/">#MapReduce</a><a href="/tags/distributed-systems/">#distributed systems</a><a href="/tags/distributed-file-system/">#distributed file system</a></p></article></div><footer><div class="paginator"><a href="/2017/08/23/introduction-to-torrenting-and-torrent-clients​/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'luyaowang';
var disqus_identifier = '2017/08/23/hadoop-big-data-best-friend/';
var disqus_title = 'Hadoop - Big Data's Best Friend';
var disqus_url = 'https://luyao-wang.github.io/2017/08/23/hadoop-big-data-best-friend/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//luyaowang.disqus.com/count.js" async></script><div class="copyright"><p>© 2017 <a href="https://luyao-wang.github.io">Luyao "Lulu" Wang & John L. Bernstein IV</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p><p>Cal Hacks 2.0 🐻 Bear Logo courtesy of : <a href="https://cal-hacks2.devpost.com">Cal Hacks 2.0@UC Berkeley</a></p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-39886095-1",'auto');ga('send','pageview');</script><link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" media="screen" type="text/css"><script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script><script>$(function(){$('.datatable').dataTable( {"order": [[ 0, "desc" ]],"iDisplayLength": -1,"lengthMenu": [[10, 25, 50, -1], [10, 25, 50, "All"]]} );});</script></body></html>